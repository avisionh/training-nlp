{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from sklearn import utils\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "from src.main import label_sentences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Document Embeddings\n",
    "Previously, we used the Word2Vec method to compute word vectors. Here, we obtained the mathematical average of the word vector representations for all the words in each document. What we want to do now is take this idea but apply it at the document level, where instead of capturing the relationship between words, we want to capture the relationship between documents.\n",
    "\n",
    "To train a Doc2Vec model, we will take a similar approach to what was done [here](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4), namely:\n",
    "\n",
    "1. Label the sentences\n",
    "    + This is because Gensim's implementation of Doc2Vec requires each document/paragraph to have a label associated to it.\n",
    "    + Will achieve by using the `TaggedDocument` method.\n",
    "1. The format will be `TRAIN_i` and `TEST_i` where `i` represents the dummy index of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
