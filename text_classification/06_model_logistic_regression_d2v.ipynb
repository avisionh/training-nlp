{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from sklearn import utils\n",
    "\n",
    "import re\n",
    "\n",
    "from src.main import label_sentences\n",
    "from src.main import get_vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Document Embeddings\n",
    "Previously, we used the Word2Vec method to compute word vectors. Here, we obtained the mathematical average of the word vector representations for all the words in each document. What we want to do now is take this idea but apply it at the document level, where instead of capturing the relationship between words, we want to capture the relationship between documents.\n",
    "\n",
    "To train a Doc2Vec model, we will take a similar approach to what was done [here](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4), namely:\n",
    "\n",
    "1. Label the sentences\n",
    "    + This is because Gensim's implementation of Doc2Vec requires each document/paragraph to have a label associated to it.\n",
    "    + Will achieve by using the `TaggedDocument` method.\n",
    "1. The format will be `TRAIN_i` and `TEST_i` where `i` represents the dummy index of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_bbc['article_text_clean'], \n",
    "                                                    df_bbc['category'],\n",
    "                                                    random_state = 42,\n",
    "                                                    test_size = 0.3)\n",
    "X_train = label_sentences(corpus = X_train, label_type = 'Train')\n",
    "X_test = label_sentences(corpus = X_test, label_type = 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['malik', 'rejects', 'black', 'mp', 'lists', 'call', 'ethnic', 'minority', 'shortlists', 'boost', 'number', 'black', 'asian', 'mps', 'rejected', 'one', 'labour', 'senior', 'asians', 'shahid', 'malik', 'labour', 'ruling', 'nec', 'accepted', 'people', 'frustration', 'said', 'targets', 'lists', 'boost', 'representation', 'minorities', '13', 'britain', '659', 'mps', 'ethnic', 'minority', 'groups', 'added', 'commission', 'racial', 'equality', 'chief', 'trevor', 'phillips', 'argued', 'sunday', 'time', 'come', 'shortlists', 'came', 'emerged', 'one', 'britain', 'ethnically', 'diverse', 'constituency', 'west', 'ham', 'get', 'women', 'shortlist', 'next', 'election', 'following', 'nec', 'ruling', 'mr', 'phillips', 'said', 'changes', 'race', 'relations', 'legislation', 'might', 'allow', 'political', 'parties', 'reserve', 'seats', 'represented', 'groups', 'example', 'west', 'ham', 'might', 'allow', 'women', 'minorities', 'seek', 'candidates', 'get', 'side', 'general', 'election', 'find', 'minorities', 'represented', 'say', '20', '30', 'years', 'talking', 'cannot', 'go', 'way', 'said', 'added', 'would', 'terribly', 'disappointing', 'least', 'white', 'constituency', 'west', 'ham', 'whole', 'europe', 'minority', 'candidate', 'appearing', 'bbc', 'radio', '4', 'today', 'programme', 'mr', 'malik', 'running', 'seat', 'dewsbury', 'acknowledged', 'far', 'women', 'shortlists', 'failed', 'deliver', 'boost', 'number', 'ethnic', 'minority', 'candidates', 'argued', 'think', 'currently', 'things', 'parties', 'example', 'setting', 'targets', 'ensure', 'existing', 'democratic', 'structures', 'reflective', 'labour', 'mp', 'diane', 'abbot', 'backs', 'mr', 'phillips', 'proposal', 'shortlists', 'said', 'elected', 'along', 'three', 'ethnic', 'minority', 'mps', 'keith', 'vaz', 'paul', 'boateng', 'bernie', 'grant', '1987', 'took', 'another', '10', 'years', 'another', 'black', 'woman', 'able', 'win', 'seat', 'rate', 'progress', 'ms', 'abbott', 'described', 'painful', 'little', 'older', 'shahid', 'served', 'national', 'executive', 'committee', '1990s', 'first', 'black', 'person', 'nec', 'crossing', 'fingers', 'hoping', 'going', 'get', 'black', 'asian', 'mp', 'worked', 'said', 'shortlist', 'strategy', 'works', 'women', 'believe', 'made', 'work', 'black', 'asian', 'people', 'tuesday', 'labour', 'chairman', 'ian', 'mccartney', 'said', 'party', 'ambitious', 'improve', 'black', 'asian', 'representation', 'ruled', 'black', 'shortlists', 'welcome', 'debate', 'party', 'said'], tags=['Train_0']),\n",
       " TaggedDocument(words=['short', 'attacks', 'us', 'tsunami', 'aid', 'former', 'cabinet', 'minister', 'clare', 'short', 'criticised', 'us', 'led', 'tsunami', 'aid', 'coalition', 'saying', 'un', 'leading', 'efforts', 'president', 'bush', 'announced', 'alliance', 'us', 'india', 'australia', 'japan', 'co', 'ordinate', 'humanitarian', 'drive', 'ms', 'short', 'said', 'effect', 'parallel', 'coalition', 'would', 'undermine', 'un', 'said', 'un', 'moral', 'authority', 'lead', 'relief', 'work', 'ms', 'short', 'resigned', 'international', 'development', 'secretary', 'iraq', 'war', 'think', 'initiative', 'america', 'set', 'four', 'countries', 'claiming', 'co', 'ordinate', 'sounds', 'like', 'yet', 'another', 'attempt', 'undermine', 'un', 'best', 'system', 'got', 'one', 'needs', 'building', 'said', 'really', 'un', 'job', 'told', 'bbc', 'radio', 'four', 'pm', 'programme', 'body', 'moral', 'authority', 'well', 'backed', 'authority', 'great', 'powers', 'ms', 'short', 'said', 'countries', 'involved', 'could', 'boast', 'good', 'records', 'response', 'major', 'disasters', 'us', 'bad', 'coordinating', 'anyone', 'india', 'problems', 'ms', 'short', 'said', 'know', 'sounds', 'much', 'afraid', 'like', 'us', 'trying', 'separate', 'operation', 'work', 'rest', 'world', 'un', 'system', 'added'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# views some docs in our train set\n",
    "data_all = X_train + X_test\n",
    "data_all[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training Doc2Vec, will vary the following parameters:\n",
    "\n",
    "- `dm = 0`: Distributed Bag of Words (DBOW) is used.\n",
    "- `vector_size = 300`: 300 dimensional feature vectors.\n",
    "- `negative = 5`: specifies how many *noise* words should be drawn\n",
    "- `min_count = 1`: ignores all word with total frequencies less than this\n",
    "- `alpha = 0.065`: the inital learning rate\n",
    "\n",
    "Initialise the model and train for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:00<00:00, 1535679.84it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm = 0, vector_size = 300, negative = 5, min_count = 1, alpha = 0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(data_all)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2225/2225 [00:00<00:00, 1662923.45it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1771512.22it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1898743.93it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2371620.43it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1924984.82it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1921022.31it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2351896.77it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2272851.05it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1903002.94it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2315139.27it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1923001.52it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1916288.79it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1938580.47it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2191715.92it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1427179.45it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1915109.05it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2274512.89it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1979284.50it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1979704.37it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1887606.47it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1333951.74it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2261285.78it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2216178.20it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1948700.44it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2360224.18it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1853859.04it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2172835.02it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 1941403.45it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2265677.69it/s]\n",
      "100%|██████████| 2225/2225 [00:00<00:00, 2356648.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(data_all)]),\n",
    "                     total_examples = len(data_all),\n",
    "                     epochs = 1)\n",
    "    model_dbow.alpha -= 0.02\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract vectors from trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model = model_dbow, \n",
    "                                 corpus_size = len(X_train),\n",
    "                                 vector_size = 300,\n",
    "                                 vector_type = 'Train')\n",
    "test_vectors_dbow = get_vectors(model = model_dbow,\n",
    "                                corpus_size = len(X_test),\n",
    "                                vector_size = 300,\n",
    "                                vector_type = 'Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use these document vectors for our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(multi_class = 'multinomial', n_jobs = 1, C = 1e5, max_iter = 15000)\n",
    "model_logreg = model_logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = model_logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.18413173652694612\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "entertainment       0.28      0.24      0.26       163\n",
      "     business       0.12      0.13      0.13       120\n",
      "        sport       0.17      0.21      0.19       112\n",
      "     politics       0.20      0.20      0.20       148\n",
      "         tech       0.13      0.11      0.12       125\n",
      "\n",
      "     accuracy                           0.18       668\n",
      "    macro avg       0.18      0.18      0.18       668\n",
      " weighted avg       0.19      0.18      0.18       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names = list_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, this is even worse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'object_keep' (dict)\n"
     ]
    }
   ],
   "source": [
    "object_keep = {'df_bbc': df_bbc,\n",
    "               'list_categories': list_categories,\n",
    "               'X': X,\n",
    "               'y': y,\n",
    "               'X_train': X_train,\n",
    "               'X_test': X_test,\n",
    "               'y_train': y_train,\n",
    "               'y_test': y_test}\n",
    "%store object_keep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
