{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import islice\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.main import word_averaging\n",
    "from src.main import word_averaging_list\n",
    "from src.main import w2v_tokenise_text\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Word Embeddings\n",
    "Thus far, have been rudimentarily counting words. Can compute word embeddings to get the relatedness of words. The point of *word embeddings* is that you can start considering the contexts of words more because you are seeking to understand words by the other words that it is surrounded by. In this regard, *word embeddings* belong to the text pre-processing stage. There are two predominant types we can choose from:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "\n",
    "Will use the word2vec model by Google which is pre-trained on 100 billion words in the Google News corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "wv.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore some vocabularies\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a `word_averaging()` function which averages two word vectors. This is the common way to average two word vectors. \n",
    "\n",
    "More generally, Bag-of-Word (BOW)-based approaches includes averaging, summation, and weighted addition.\n",
    "\n",
    "Also have created the `w2v_tokenise_text()` function which tokenises text. We will then apply this function onto the `article_text_clean` column. At this point, we will then apply word vector averaging to the tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_bbc[['article_text_clean', 'category']], test_size = 0.3, random_state = 42)\n",
    "\n",
    "train_tokenised = train.apply(lambda r: w2v_tokenise_text(r['article_text_clean']),\n",
    "                             axis = 1).values\n",
    "test_tokenised = test.apply(lambda r: w2v_tokenise_text(r['article_text_clean']),\n",
    "                           axis = 1).values\n",
    "\n",
    "#X_train_word_average = word_averaging_list(wv, train_tokenised)\n",
    "#X_test_word_average = word_averaging_list(wv, test_tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Now let's see how the logistic regression classifier performs on these word-averaging document features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(multi_class = 'multinomial', n_jobs = 1, C = 1e5, max_iter = 4000)\n",
    "model_logreg = model_logreg.fit(X_train_word_average, train['category'])\n",
    "y_pred = model_logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.572\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "entertainment       0.59      0.51      0.55       163\n",
      "     business       0.57      0.43      0.49       120\n",
      "        sport       0.60      0.61      0.60       112\n",
      "     politics       0.55      0.78      0.65       148\n",
      "         tech       0.56      0.50      0.53       125\n",
      "\n",
      "     accuracy                           0.57       668\n",
      "    macro avg       0.57      0.57      0.56       668\n",
      " weighted avg       0.57      0.57      0.57       668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_pred, test['category'])))\n",
    "print(classification_report(test['category'], y_pred, target_names = list_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Our accruacy has drastically fallen from 98.4% to 57.2%! That's pitiful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_keep = {'df_bbc': df_bbc,\n",
    "               'list_categories': list_categories,\n",
    "               'X': X,\n",
    "               'y': y,\n",
    "               'X_train': X_train,\n",
    "               'X_test': X_test,\n",
    "               'y_train': y_train,\n",
    "               'y_test': y_test}\n",
    "%store object_keep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
