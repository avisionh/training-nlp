{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import islice\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.main import word_averaging\n",
    "from src.main import word_averaging_list\n",
    "from src.main import w2v_tokenise_text\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Word Embeddings\n",
    "Thus far, have been rudimentarily counting words. Can compute word embeddings to get the relatedness of words. The point of *word embeddings* is that you can start considering the contexts of words more because you are seeking to understand words by the other words that it is surrounded by. In this regard, *word embeddings* belong to the text pre-processing stage. There are two predominant types we can choose from:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "\n",
    "Will use the word2vec model by Google which is pre-trained on 100 billion words in the Google News corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "wv.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Memorial_Hospital',\n",
       " 'Seniors',\n",
       " 'memorandum',\n",
       " 'elephant',\n",
       " 'Trump',\n",
       " 'Census',\n",
       " 'pilgrims',\n",
       " 'De',\n",
       " 'Dogs',\n",
       " '###-####_ext',\n",
       " 'chaotic',\n",
       " 'forgive',\n",
       " 'scholar',\n",
       " 'Lottery',\n",
       " 'decreasing',\n",
       " 'Supervisor',\n",
       " 'fundamentally',\n",
       " 'Fitness',\n",
       " 'abundance',\n",
       " 'Hold']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore some vocabularies\n",
    "list(islice(wv.vocab, 13030, 13050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a `word_averaging()` function which averages two word vectors. This is the common way to average two word vectors. \n",
    "\n",
    "More generally, Bag-of-Word (BOW)-based approaches includes averaging, summation, and weighted addition.\n",
    "\n",
    "Also have created the `w2v_tokenise_text()` function which tokenises text. We will then apply this function onto the `article_text_clean` column. At this point, we will then apply word vector averaging to the tokenised text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_bbc[['article_text_clean', 'category']], test_size = 0.3, random_state = 42)\n",
    "\n",
    "train_tokenised = train.apply(lambda r: w2v_tokenise_text(r['article_text_clean']),\n",
    "                             axis = 1).values\n",
    "test_tokenised = test.apply(lambda r: w2v_tokenise_text(r['article_text_clean']),\n",
    "                           axis = 1).values\n",
    "\n",
    "#X_train_word_average = word_averaging_list(wv, train_tokenised)\n",
    "#X_test_word_average = word_averaging_list(wv, test_tokenised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "Now let's see how the logistic regression classifier performs on these word-averaging document features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0895576 ,  0.02565143,  0.00313229, ...,  0.03484443,\n",
       "         0.06731648, -0.01771876],\n",
       "       [ 0.05634755,  0.02117012,  0.00037306, ..., -0.04679712,\n",
       "         0.01289308,  0.03358569],\n",
       "       [-0.04414319,  0.00448789,  0.05503185, ...,  0.01927586,\n",
       "        -0.05179468,  0.04208318],\n",
       "       ...,\n",
       "       [ 0.03414855,  0.00432295, -0.0356711 , ...,  0.01500796,\n",
       "        -0.03980372, -0.00995093],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.03754066, -0.01188214,  0.00955737, ..., -0.02617514,\n",
       "         0.01162383,  0.03771286]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
