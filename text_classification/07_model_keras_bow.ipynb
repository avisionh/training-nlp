{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Bag of Words\n",
    "\n",
    "Will now use the deep-learning framework, [Keras](https://keras.io/), to perform our text classification.\n",
    "\n",
    "The process will be like so:\n",
    "1. Separate the data into the training and test sets.\n",
    "1. Use the `tokenizer` method to count the unique words in our vocabulary and assign each of these words to indices.\n",
    "1. Call `fit_on_texts()` automatically creates a word index lookup of our vocabulary.\n",
    "1. Limit our vocabulary to the top words by passing a `num_words` parameter to the `tokenizer` method.\n",
    "1. With the `tokenizer` method, can use the `texts_to_matrix` method to create the training data that we'll pass to our model.\n",
    "1. Pass a one-hot vector to our model.\n",
    "1. Transform our features and labels into a format that Keras can read.\n",
    "1. Build our model, telling Keras the shape of our:\n",
    "    + input data\n",
    "    + output data\n",
    "    + type of each layer\n",
    "1. When training the model, call the `fit()` method, pass the training data and labels, batch size and epochs.\n",
    "\n",
    "> *Note*: Generally, deep-learning works best when you have tonnes of data, probably above 10,000. In our case, we don't but we still proceed with this deep-learning to get used to using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
