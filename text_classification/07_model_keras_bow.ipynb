{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "# restore objects and unpack them into variables\n",
    "%store -r object_keep\n",
    "df_bbc, list_categories, X, y, X_train, X_test, y_train, y_test = itemgetter('df_bbc',\n",
    "                                                                             'list_categories',\n",
    "                                                                             'X',\n",
    "                                                                             'y',\n",
    "                                                                             'X_train',\n",
    "                                                                             'X_test',\n",
    "                                                                             'y_train',\n",
    "                                                                             'y_test')(object_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Bag of Words\n",
    "\n",
    "Will now use the deep-learning framework, [Keras](https://keras.io/), to perform our text classification.\n",
    "\n",
    "The process will be like so:\n",
    "1. Separate the data into the training and test sets.\n",
    "1. Use the `tokenizer` method to count the unique words in our vocabulary and assign each of these words to indices.\n",
    "1. Call `fit_on_texts()` automatically creates a word index lookup of our vocabulary.\n",
    "1. Limit our vocabulary to the top words by passing a `num_words` parameter to the `tokenizer` method.\n",
    "1. With the `tokenizer` method, can use the `texts_to_matrix` method to create the training data that we'll pass to our model.\n",
    "1. Pass a one-hot vector to our model.\n",
    "1. Transform our features and labels into a format that Keras can read.\n",
    "1. Build our model, telling Keras the shape of our:\n",
    "    + input data\n",
    "    + output data\n",
    "    + type of each layer\n",
    "1. When training the model, call the `fit()` method, pass the training data and labels, batch size and epochs.\n",
    "\n",
    "> *Note*: Generally, deep-learning works best when you have tonnes of data, probably above 10,000. In our case, we don't but we still proceed with this deep-learning to get used to using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>article_text</th>\n",
       "      <th>article_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>business</td>\n",
       "      <td>b\"China now top trader with Japan\\n\\nChina ove...</td>\n",
       "      <td>china top trader japan china overtook us becom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>business</td>\n",
       "      <td>b'Bush budget seeks deep cutbacks\\n\\nPresident...</td>\n",
       "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>politics</td>\n",
       "      <td>b'MPs\\' murder sentence concern\\n\\nMurder sent...</td>\n",
       "      <td>mps murder sentence concern murder sentences r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>business</td>\n",
       "      <td>b'GE sees \\'excellent\\' world economy\\n\\nUS be...</td>\n",
       "      <td>ge sees excellent world economy us behemoth ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>sport</td>\n",
       "      <td>b'Rush future at Chester uncertain\\n\\nIan Rush...</td>\n",
       "      <td>rush future chester uncertain ian rush future ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>politics</td>\n",
       "      <td>b'Labour\\'s Cunningham to stand down\\n\\nVetera...</td>\n",
       "      <td>labour cunningham stand veteran labour mp form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>sport</td>\n",
       "      <td>b'Collins to compete in Birmingham\\n\\nWorld an...</td>\n",
       "      <td>collins compete birmingham world commonwealth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>sport</td>\n",
       "      <td>b'Juninho demand for O\\'Neill talks\\n\\nJuninho...</td>\n",
       "      <td>juninho demand neill talks juninho agent confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>sport</td>\n",
       "      <td>b'Wenger shock at Newcastle dip\\n\\nArsenal man...</td>\n",
       "      <td>wenger shock newcastle dip arsenal manager ars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>business</td>\n",
       "      <td>b'BT offers equal access to rivals\\n\\nBT has m...</td>\n",
       "      <td>bt offers equal access rivals bt moved pre emp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                       article_text  \\\n",
       "28   business  b\"China now top trader with Japan\\n\\nChina ove...   \n",
       "34   business  b'Bush budget seeks deep cutbacks\\n\\nPresident...   \n",
       "237  politics  b'MPs\\' murder sentence concern\\n\\nMurder sent...   \n",
       "30   business  b'GE sees \\'excellent\\' world economy\\n\\nUS be...   \n",
       "336     sport  b'Rush future at Chester uncertain\\n\\nIan Rush...   \n",
       "..        ...                                                ...   \n",
       "231  politics  b'Labour\\'s Cunningham to stand down\\n\\nVetera...   \n",
       "199     sport  b'Collins to compete in Birmingham\\n\\nWorld an...   \n",
       "234     sport  b'Juninho demand for O\\'Neill talks\\n\\nJuninho...   \n",
       "398     sport  b'Wenger shock at Newcastle dip\\n\\nArsenal man...   \n",
       "474  business  b'BT offers equal access to rivals\\n\\nBT has m...   \n",
       "\n",
       "                                    article_text_clean  \n",
       "28   china top trader japan china overtook us becom...  \n",
       "34   bush budget seeks deep cutbacks president bush...  \n",
       "237  mps murder sentence concern murder sentences r...  \n",
       "30   ge sees excellent world economy us behemoth ge...  \n",
       "336  rush future chester uncertain ian rush future ...  \n",
       "..                                                 ...  \n",
       "231  labour cunningham stand veteran labour mp form...  \n",
       "199  collins compete birmingham world commonwealth ...  \n",
       "234  juninho demand neill talks juninho agent confi...  \n",
       "398  wenger shock newcastle dip arsenal manager ars...  \n",
       "474  bt offers equal access rivals bt moved pre emp...  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbc_shuffle = shuffle(df_bbc, random_state = 42)\n",
    "df_bbc_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n",
    "train_size = int(len(df_bbc) * 0.7)\n",
    "train_articles = df_bbc_shuffle['article_text_clean'][:train_size]\n",
    "train_labels = df_bbc_shuffle['category'][:train_size]\n",
    "\n",
    "test_articles = df_bbc_shuffle['article_text_clean'][train_size:]\n",
    "test_labels = df_bbc_shuffle['category'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise\n",
    "max_words = 1000\n",
    "tokenise = text.Tokenizer(num_words = max_words, char_level = False)\n",
    "tokenise.fit_on_texts(train_articles)\n",
    "\n",
    "X_train = tokenise.texts_to_matrix(train_articles)\n",
    "X_test = tokenise.texts_to_matrix(test_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_labels)\n",
    "\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 512, input_shape = (max_words, )))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "44/44 [==============================] - 0s 6ms/step - loss: 0.6657 - accuracy: 0.8151 - val_loss: 0.1877 - val_accuracy: 0.9423\n",
      "Epoch 2/2\n",
      "44/44 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9729 - val_loss: 0.1203 - val_accuracy: 0.9679\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose = 1,\n",
    "                    validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9701\n",
      "Test accuracy:  0.970059871673584\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test,\n",
    "                       batch_size = batch_size, \n",
    "                       verbose = 1)\n",
    "print(\"Test accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, not bad. 97% is pretty decent. In fact, it's quite surprising given how relatively small our dataset is for a deep-learning problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "env_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
